{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytorch modules\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn import init\n",
    "import functools\n",
    "from torch.optim import lr_scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder3d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder3d, self).__init__()\n",
    "        \n",
    "        # hyperparameters\n",
    "        self.z_size = 200\n",
    "        self.cube_len = 32\n",
    "        self.bias = True\n",
    "\n",
    "        padd = (0, 0, 0)\n",
    "\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose3d(self.z_size, self.cube_len*4, kernel_size=4, stride=2,\\\n",
    "                                     bias=self.bias, padding=padd),\n",
    "            torch.nn.BatchNorm3d(self.cube_len*4),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose3d(self.cube_len*4, self.cube_len*2, kernel_size=4, stride=2, bias=self.bias,\\\n",
    "                                     padding=(1, 1, 1)),\n",
    "            torch.nn.BatchNorm3d(self.cube_len*2),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose3d(self.cube_len*2, self.cube_len*1, kernel_size=4, stride=2, bias=self.bias,\\\n",
    "                                     padding=(1, 1, 1)),\n",
    "            torch.nn.BatchNorm3d(self.cube_len*1),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose3d(self.cube_len, 1, kernel_size=4, stride=2, bias=self.bias, padding=(1, 1, 1)),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x.unsqueeze(-1)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "#         print(out.size())\n",
    "        out = self.layer3(out)\n",
    "#         print(out.size())\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder3d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder3d, self).__init__()\n",
    "        \n",
    "        # hyperparameters\n",
    "        self.z_size = 200\n",
    "        self.cube_len = 32\n",
    "        self.bias = True\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv3d(1, self.cube_len, kernel_size=4, stride=2, bias=self.bias, padding=(1, 1, 1)),\n",
    "            nn.BatchNorm3d(self.cube_len*1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv3d(self.cube_len, self.cube_len*2, kernel_size=4, stride=2, bias=self.bias,\\\n",
    "                                     padding=(1, 1, 1)),\n",
    "            nn.BatchNorm3d(self.cube_len*2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv3d(self.cube_len*2, self.cube_len*4, kernel_size=4, stride=2, bias=self.bias,\\\n",
    "                                     padding=(1, 1, 1)),\n",
    "            nn.BatchNorm3d(self.cube_len*4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            nn.Conv3d(self.cube_len*4, self.z_size, kernel_size=4, stride=2,\\\n",
    "                                     bias=self.bias, padding=(0,0,0)),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder2d(nn.Module):\n",
    "    def __init__(self, input_nc=1, output_nc=200, ngf=8, norm_layer=nn.BatchNorm2d,\\\n",
    "                 use_dropout=False, padding_type='reflect'):\n",
    "        super(Encoder2d, self).__init__()\n",
    "        self.input_nc = input_nc\n",
    "        self.output_nc = output_nc\n",
    "        self.ngf = ngf\n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "\n",
    "        model = [nn.ReflectionPad2d(2),\n",
    "                 nn.Conv2d(input_nc, ngf, kernel_size=5, padding=0,\n",
    "                           bias=use_bias),\n",
    "                 norm_layer(ngf),\n",
    "                 nn.ReLU(True)]\n",
    "        \n",
    "        \n",
    "        n_downsampling = 4\n",
    "        for i in range(n_downsampling):\n",
    "            mult = 2**i\n",
    "            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=5,\n",
    "                                stride=2, padding=1, bias=use_bias),\n",
    "                      norm_layer(ngf * mult * 2),\n",
    "                      nn.ReLU(True)]\n",
    "        \n",
    "        for i in range(2):\n",
    "            model += [nn.Conv2d(ngf * mult * 2, ngf * mult * 2, kernel_size=3,\n",
    "                                    stride=2, padding=1, bias=use_bias),\n",
    "                          norm_layer(ngf * mult * 2),\n",
    "                          nn.ReLU(True)]\n",
    "            \n",
    "        model += [nn.Conv2d(ngf * mult * 2, output_nc, kernel_size=2,\n",
    "                                    stride=1, padding=0, bias=use_bias)]\n",
    "        model += [nn.Tanh()]\n",
    "        \n",
    "            \n",
    "        \n",
    "\n",
    "#         mult = 2**(n_downsampling-1)\n",
    "#         for i in range(n_blocks):\n",
    "#             model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer,\\\n",
    "#                                   use_dropout=use_dropout, use_bias=use_bias)]\n",
    "            \n",
    "        self.model = nn.ModuleList(model)\n",
    "#         self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for model in self.model:\n",
    "            x = model(x)\n",
    "#             print(x.size())\n",
    "        return x\n",
    "        #return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder2d(nn.Module):\n",
    "    def __init__(self, input_nc=200, output_nc=1, ngf=8, norm_layer=nn.BatchNorm2d,\\\n",
    "                 use_dropout=False, padding_type='reflect'):\n",
    "        super(Decoder2d, self).__init__()\n",
    "        self.input_nc = input_nc\n",
    "        self.output_nc = output_nc\n",
    "        self.ngf = ngf\n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "\n",
    "       \n",
    "\n",
    "        model = []\n",
    "        \n",
    "        n_upsampling = 4\n",
    "        mult = 2**(n_upsampling-1)\n",
    "        \n",
    "        model += [nn.ConvTranspose2d(input_nc, ngf * mult * 2, kernel_size=2,\n",
    "                                    stride=1, padding=0, bias=use_bias),\\\n",
    "                                norm_layer(ngf * mult * 2),\n",
    "                                nn.ReLU(True)]\n",
    "        \n",
    "        \n",
    "        for i in range(2):\n",
    "            model += [nn.ConvTranspose2d(ngf * mult * 2, ngf * mult * 2, kernel_size=3,\n",
    "                                        stride=2, padding=1, bias=use_bias),\n",
    "                              norm_layer(ngf * mult * 2),\n",
    "                              nn.ReLU(True)]\n",
    "            \n",
    "        # padding to maintain output size\n",
    "        p2d = (1, 1, 1, 1) \n",
    "        model += [nn.ZeroPad2d(p2d),]\n",
    "        \n",
    "        for i in range(n_upsampling):\n",
    "            mult = 2**(n_upsampling-1-i)\n",
    "            model += [nn.ConvTranspose2d(ngf * mult * 2, ngf * mult, kernel_size=5,\n",
    "                                stride=2, padding=1, bias=use_bias),\\\n",
    "                      norm_layer(ngf * mult),\n",
    "                      nn.ReLU(True)]\n",
    "        \n",
    "        # padding to maintain output size\n",
    "        p2d = (0, 1, 0, 1) \n",
    "        model += [nn.ZeroPad2d(p2d),]\n",
    "        \n",
    "        model += [#nn.ReflectionPad2d(2),\n",
    "                 nn.ConvTranspose2d(ngf, output_nc, kernel_size=5, padding=2,\n",
    "                           bias=use_bias)]\n",
    "        \n",
    "#         self.model = nn.Sequential(*model)\n",
    "        self.model = nn.ModuleList(model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = x.squeeze(-1)\n",
    "        for model in self.model:\n",
    "            out = model(out)\n",
    "#             print(out.size())\n",
    "#         out = self.model(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.load('padded_gray_images.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import test data\n",
    "img = images[0]\n",
    "img = Variable(torch.from_numpy(img).view(1,1, *img.shape).type(torch.FloatTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "_E2d = Encoder2d()\n",
    "_D3d = Decoder3d()\n",
    "_E3d = Encoder3d()\n",
    "_D2d = Decoder2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200, 1, 1])\n",
      "torch.Size([1, 1, 32, 32, 32])\n",
      "torch.Size([1, 200, 1, 1, 1])\n",
      "torch.Size([1, 1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "code = _E2d(img)\n",
    "print(code.size())\n",
    "output = _D3d(code)\n",
    "print(output.size())\n",
    "code = _E3d(output)\n",
    "print(code.size())\n",
    "output = _D2d(code)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
